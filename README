This is a prototype of a clustered system built with Lucy


The system should work by distributing a Lucy index on many nodes. 
The main differences in the interface a user will see is.

1. The enforcement of a primary_key in the schema.
2. You can't use delete_by_query so deletes need to be 
   done with the primary_key
3. Must know how to reach 1+ cluster_server endpoints.



The Basic Idea.

Search:

Create N virtual shards (Standard Lucy Indexes). Distribute them over Y nodes (physical servers).
So lets say N=30, Y=3. We would place 10 shards on each of the 3 boxes. Then performer Local 
multi-searches on the 10 indexes (async) sending the results for all 10 shards back as a single result 
for each of the 3 nodes (async). I'm looking to see how fast I can do the 10 local searches and sync 
them with the other 2 nodes.. 

Cluster configuration is handled from the bottom up. What I mean by this is the Lucy indexes on disk
can appear from any number of techniques. Starting up a cluster_searcher and pointing him to a 
cluster aggregator should be all that is needed to expose a index to a user. Aggregator processes
may be stacked. An aggregator ONLY talks to people  below him and people above him. NEVER his peers.
Users May only connect up and query a aggregator that has no-one above him.



Index:

A user that does a write should have the ability to see his write on the next search. *If* he
waits for the write to finish. One way I can think of to do this
is "search sessions". In a system where we are using style (A) replication, users doing searchers could
hint to the query system the last time *they* made a change. Letting the system pick shards that are 
new enough to properly service the request. In a style (B) search sessions would be tied to the shards
that user wrote forcing them to return there until other shards have time to catch up.
Giving the system a perception of Real Time Search (you change it you see it).



Replication:

  Oh replication... This is tricky.. 
  We have to deal with stored and non-stored fields, primary_keys or shard_keys, and maybe doc versions!
  This leaves us with only a few good options I think.

  A. Replicate the index files on disk to another server. 
    (http://wiki.apache.org/solr/SolrReplication)
    On updates/additions you still end up paying the primary_key cost on the master's write.

  B. Replicate indexing sessions with doc versions
    Lets define a indexing session as the log of all 
    actions a user took from the time they "opened" the index till the time they called commit.
    You bundle each doc with with a version (hi-res timestamps).
    Any doc found in the index thats newer than the doc in the session is ignored.
    This should let us run the indexing sessions out of order and make multi-master a option.

    The largest issue with this style of replication is write performance. 
    Enforcing primary_key + version checking on every doc per node 
    (each node does the same work)..

  C. Replicate indexing sessions without doc versions 
    Pretty much the same as above but the sessions must be played in order.
    We still end up paying the primary_key cost but don't have to use versions



\bin\
  cluster_server.pl (1 per server) 
    This is the users door into the system. Hands out schema's, Maintains a list of 
    nodes/shards and other cluster_servers? 

  cluster_write_node.pl (N per cluster_node)
    
  cluster_search_node.pl  (~1 per-core)
    This guy is our async multi-searcher using cluster_searchers.
  
  cluster_searcher.pl (N per cluster_node)
    This guy does blocking searches on the indexes at the cluster_nodes request
    (the index being multi-searched)



